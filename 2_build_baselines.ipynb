{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_build_baselines\n",
    "\n",
    "Notebook to build baseline models to iterate on a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from supervised.automl import AutoML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import wandb\n",
    "\n",
    "evaluation_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the data\n",
    "dfp_train = pd.read_csv('./data/rtu/model_train_data.csv')\n",
    "dfp_train['date'] = pd.to_datetime(dfp_train['date'])\n",
    "\n",
    "dfp_test = pd.read_csv('./data/rtu/model_test_data.csv')\n",
    "dfp_test['date'] = pd.to_datetime(dfp_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defione the columns and features\n",
    "columns_weather = [ 't2m_min_bordeaux',\n",
    "       't2m_bordeaux', 't2m_max_bordeaux', 'prectot_bordeaux', 't2m_min_lille',\n",
    "       't2m_lille', 't2m_max_lille', 'prectot_lille', 't2m_min_paris',\n",
    "       't2m_paris', 't2m_max_paris', 'prectot_paris', 't2m_min_rennes',\n",
    "       't2m_rennes', 't2m_max_rennes', 'prectot_rennes', 't2m_min_nantes',\n",
    "       't2m_nantes', 't2m_max_nantes', 'prectot_nantes', 't2m_min_toulouse',\n",
    "       't2m_toulouse', 't2m_max_toulouse', 'prectot_toulouse',\n",
    "       't2m_min_marseille', 't2m_marseille', 't2m_max_marseille',\n",
    "       'prectot_marseille', 't2m_min_lyon', 't2m_lyon', 't2m_max_lyon',\n",
    "       'prectot_lyon', 't2m_min_nice', 't2m_nice', 't2m_max_nice',\n",
    "       'prectot_nice', 't2m_min_strasbourg', 't2m_strasbourg',\n",
    "       't2m_max_strasbourg', 'prectot_strasbourg', 't2m_min_montpellier',\n",
    "       't2m_montpellier', 't2m_max_montpellier', 'prectot_montpellier',\n",
    "       'weighted_t2m', 'weighted_t2m_min', 'weighted_t2m_max',\n",
    "       'weighted_prectot']\n",
    "\n",
    "columns_features = ['weekday', 'month', 'week_number'] + columns_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_target = 'daily_electrical_consumption'\n",
    "X_train, y_train = dfp_train[columns_features], dfp_train[column_target]\n",
    "X_test, y_test = dfp_test[columns_features], dfp_test[column_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test-set: 970609.7624346911\n"
     ]
    }
   ],
   "source": [
    "# First baseline\n",
    "predictions = [random.randrange(y_train.min(), y_train.max()) for idx in range(len(y_test))]\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "evaluation_metrics.append({'model' : 'random', 'rmse' : rmse})\n",
    "print('RMSE on the test-set:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test-set: 971457.4083803146\n"
     ]
    }
   ],
   "source": [
    "# Second baseline, still a little bit random but a little bit of knowledge based\n",
    "dfp_tmp = dfp_train.groupby(['month', 'weekday']).agg({column_target : ['min', 'max']})\n",
    "dfp_tmp.columns = ['min', 'max']\n",
    "dfp_tmp.reset_index(inplace=True)\n",
    "\n",
    "dfp_tmp['key'] = dfp_tmp.apply(lambda row: f\"{row['month']}-{row['weekday']}\", axis=1)\n",
    "dfp_tmp.set_index('key', drop=True, inplace=True)\n",
    "\n",
    "dict_knowledge = dfp_tmp[['min', 'max']].to_dict(orient='index')\n",
    "dict_knowledge['x-x'] = {'min' : y_train.min(), 'max' : y_train.max()}\n",
    "\n",
    "def get_randomish_consumption(month, weekday, dict_knowledge):\n",
    "    \n",
    "    key = f\"{month}-{weekday}\"\n",
    "    if key not in dict_knowledge:\n",
    "        key = 'x-x'\n",
    "    return random.randrange(dict_knowledge[key]['min'], dict_knowledge[key]['max'])\n",
    "   \n",
    "predictions = [get_randomish_consumption(row['month'], row['weekday'], dict_knowledge) for idx, row in dfp_test.iterrows()] \n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "evaluation_metrics.append({'model' : 'randomish', 'rmse' : rmse})\n",
    "print('RMSE on the test-set:', rmse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_1\n",
      "The task is regression with evaluation metric rmse\n",
      "AutoML will use algorithms: ['Baseline', 'Linear', 'Decision Tree', 'Random Forest', 'Xgboost', 'Neural Network']\n",
      "AutoML will ensemble availabe models\n",
      "AutoML steps: ['simple_algorithms', 'default_algorithms', 'ensemble']\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "1_Baseline rmse 515197.30308 trained in 0.38 seconds\n",
      "2_DecisionTree rmse 433299.458476 trained in 7.9 seconds\n",
      "3_Linear rmse 439952.141777 trained in 2.91 seconds\n",
      "* Step default_algorithms will try to check up to 3 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_Default_Xgboost rmse 405015.521824 trained in 9.55 seconds\n",
      "5_Default_NeuralNetwork rmse 430172.411576 trained in 0.94 seconds\n",
      "6_Default_RandomForest rmse 416696.642759 trained in 6.55 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble rmse 402723.43411 trained in 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An input array is constant; the correlation coefficent is not defined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML fit time: 43.76 seconds\n",
      "AutoML best model: Ensemble\n",
      "RMSE on the test-set: 425946.06338705606\n",
      "CPU times: user 1min 5s, sys: 1.25 s, total: 1min 6s\n",
      "Wall time: 44 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "automl = AutoML() # mode=Explain, Perform, Compete\n",
    "automl.fit(X_train, y_train)\n",
    "predictions = automl.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "evaluation_metrics.append({'model' : 'mljar-bm', 'rmse' : rmse})\n",
    "print('RMSE on the test-set:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test-set: 473806.71499787306\n"
     ]
    }
   ],
   "source": [
    "def ptg_model(x, a, b, x0):\n",
    "    return np.piecewise(x, [x < x0, x >= x0], [lambda x: a*x+b , lambda x:a*x0+b])\n",
    "\n",
    "def get_model_ptg(x,y):\n",
    "    x0_min = 0\n",
    "    x0_max = 20\n",
    "    a_min=-200000\n",
    "    a_max=-50000\n",
    "    b_min=1000000\n",
    "    b_max=3000000\n",
    "    bounds_min = [a_min, b_min, x0_min]\n",
    "    bounds_max = [a_max, b_max, x0_max]\n",
    "    bounds = (bounds_min, bounds_max)\n",
    "    popt, pcov = scipy.optimize.curve_fit(ptg_model, x, y, bounds=bounds)\n",
    "    a= popt[0]\n",
    "    b = popt[1]\n",
    "    x0 = popt[2]\n",
    "    return a,b,x0\n",
    "\n",
    "a, b, x0 = get_model_ptg(X_train['weighted_t2m'].tolist(),y_train)\n",
    "predictions = [ptg_model(row['weighted_t2m'], a, b, x0) for idx, row in dfp_test.iterrows()] \n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "evaluation_metrics.append({'model' : 'ptg-wt2m', 'rmse' : rmse})\n",
    "print('RMSE on the test-set:', rmse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test-set: 463374.5758606644\n"
     ]
    }
   ],
   "source": [
    "a, b, x0 = get_model_ptg(X_train['weighted_t2m_min'].tolist(),y_train)\n",
    "predictions = [ptg_model(row['weighted_t2m_min'], a, b, x0) for idx, row in dfp_test.iterrows()] \n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "evaluation_metrics.append({'model' : 'ptg-wt2mmin', 'rmse' : rmse})\n",
    "print('RMSE on the test-set:', rmse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test-set: 507987.698223197\n"
     ]
    }
   ],
   "source": [
    "a, b, x0 = get_model_ptg(X_train['weighted_t2m_max'].tolist(),y_train)\n",
    "predictions = [ptg_model(row['weighted_t2m_max'], a, b, x0) for idx, row in dfp_test.iterrows()] \n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "evaluation_metrics.append({'model' : 'ptg-wt2mmax', 'rmse' : rmse})\n",
    "print('RMSE on the test-set:', rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_evaluation_metrics = pd.DataFrame(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-26 11:39:20,205 wandb.jupyter ERROR Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjmdaignan\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.32 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">decent-salad-341</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jmdaignan/french_electrical_consumption\" target=\"_blank\">https://wandb.ai/jmdaignan/french_electrical_consumption</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jmdaignan/french_electrical_consumption/runs/2j118pmg\" target=\"_blank\">https://wandb.ai/jmdaignan/french_electrical_consumption/runs/2j118pmg</a><br/>\n",
       "                Run data is saved locally in <code>/home/jmdaignan/Development/shotgun-20210627/wandb/run-20210626_113920-2j118pmg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='french_electrical_consumption', entity='jmdaignan')\n",
    "data = [[row['model'], row['rmse']] for idx, row in dfp_evaluation_metrics.iterrows()]\n",
    "table = wandb.Table(data=data, columns = [\"model\", \"rmse\"])\n",
    "wandb.log({\"comparison_baseline\" : wandb.plot.bar(table, \"model\", \"rmse\", title=\"Comparison baseline models\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oldml_p37",
   "language": "python",
   "name": "oldml_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
